\section{Results}

\subsection{Single-shot model performance}
\label{subsection:single-shot}

Firstly, it is clear that the tested models have performed differently on different datasets assessed in this study (Figure \ref{fig:fig_3_singleshot}). Namely, for the datasets where the scores were obtained with DOCK (D4 and AmpC), recall scores tend to be higher than for those obtained with ICM.

Secondly, it seems that the increase in the training dataset size does not lead to significant improvements in the models performance, in line with previously observed results \cite{Yang2021_shoichet_active_learning}. Despite that, "heavyweight" models such as RandomForestRegressor, seem to benefit by that more, compared to more "lightweight" linear models, which seem to saturate at train size around $160\ 000$ \ref{fig:fig_3_singleshot}.

Linear models also show more stable performance across the datasets: namely, default LinearRegression and LinearSVR show recall score similar to that of RandomForestRegressor on all datasets except for CB2, where LinearRegression shows twice smaller recall values compared to RandomForestRegressor. Interestingly, adding regularization to linear models does not increase their performance, as seen by LassoCV and RidgeCV performance.

Notably, the execution time (Figure \ref{fig:supp_fig_1_execution_time}) for the linear regression remains within few minutes, whereas more "heavyweight" algorithms take up to 100 times more for the train-predict loop even on a million-sized library, while previous works reported up to 1 day of train-predict time using modern GPUs \cite{deepdocking}.

\subsection{Extrapolation of single-shot results}
Following the robust performance of LinearRegression in single-shot regime, we compared its active learning regime with its extrapolation from a single-shot performance.

Figure \ref{fig:fig_4_extrapolation}) shows the results: it is clear that for the large batch size (40 000 or 20 000), extrapolation can reliably predict the outcome of the memory-less active learning. However, with the decrease of the batch size, the extrapolation seems to be overestimating the performance. Despite that, the superior performance of active learning with smaller batch size is still obvious.

It is worth noticing that results of the simple active learning model go in line with the previous results (Graff et al. \cite{Graff2021AcceleratingLearning}, Figure 5): percentage of the top-1000 molecules found increases with the increase in the step size, at the same number of molecules that were docked. We find that the absolute number of docked ligands, and not the number of active learning steps, is more suitable scale in this case, since the docking itself is, especially in our case of lightweight models, is the most time-consuming step.

Interestingly, when using second docking run as a docking score predictor, it shows superior performance in the first few iterations, but then its predictions yield less real hits per step than the random search (Figure \ref{fig:fig_4_extrapolation}, datasets AA2AR and CB2). This is likely due to the fact that the reliably predicted VSHs are quickly exhausted at the first few steps, and the second docking is effectively useless for ligands with lower scores. Potentially, comparing success of the active learning batch prediction with the random batch prediction might serve as a stopping criteria for the real accelerated docking screening campaign.

\subsection{Optimal parameters of the active learning regime}
The meta-parameters of the active regime focused on adding memory of the docking results between different batches, as well as increasing the training size between batches. Here we discuss the \textit{early} recall (percentage of VSHs obtained after docking approx. 10\% of the library) and \textit{late} recall (after docking 30\% of the library).

Models perform drastically differrent with different datasets: while for AmpC dataset, around 90\% of the VSHs are found already after screening the first 10\%, for the CB2 dataset even the best late recall is around 80\%, also requiring three times as much ligands docked. Besides, the relative model performances between different datasets are more clear.

For different meta-parameters the decrease of the training size is also beneficial here (Figure \ref{tab:tab_1_activelearning}), in agreement to the memory-less regime and the extrapolation of single-shot results. For different datasets, the smallest vs. largest batch size (40 000 vs 8 000) results in 2-3 times difference in the early recall, although difference in late recall is less significant.

Adding simple model ensembling via either MeanRank or TopFromEveryModel mechanism, or simply increasing train size (LastModel with "add" parameter) confidently boosts performance: both early and late recall is 10-15 percent points higher for all the models with a memory mechanism.

However, differences in model performance with different memory mechanisms are less obvious. If we focus on the "noadd" regime, that keeps the training size constant between the batches, we can see that at batch size 8000  ensembling regime MeanRank has better performance compared to two other methods. In this regime, each base model learns a piece of valuable information about its chemical subspace, and low-ranked molecules from a single base model still can end up in the final list of hits for the next iteration.

Interestingly, gradually increasing the train size does not boost the overall performance of the active learning regime. While it does increase performance for the LastModel ensembling, effectively adding memory to the model, it does not increase the overall performance of other models, implicitly having memory mechanisms. It agrees well with the single-shot performance results and the choice of simple LinearRegression models, that seem to saturate in their performance at around $16\ 000$ batch size.

Also, even though the memory-less LastModel regime shows worse performance compared to others, it is still considerably higher than the performance of the random choice screening. For example, for the least performative CB2 dataset, it still finds around 34\% of the VSHs after screening only 10\% of ligands, around half of the VSHs for D4 and AA2AR datasets, and 86\% VSHs for the AmpC dataset.
